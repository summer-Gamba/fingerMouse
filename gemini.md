얼굴 인식 및 손 제스처 기반 스마트 인터페이스 시스템 개발 프로젝트
1. 프로젝트 개요 (Introduction)
1.1. 프로젝트 목적
본 프로젝트의 최종 목표는 라즈베리파이 5와 카메라 모듈을 활용하여, 별도의 입력 장치 없이 사용자의 얼굴과 손 제스처만으로 컴퓨터와 상호작용할 수 있는 스마트 인터페이스 시스템을 개발하는 것입니다. 이 시스템은 다음과 같은 구체적인 기능을 포함합니다.

얼굴 인식 기반 로그인: 등록된 사용자의 얼굴을 인식하여 시스템 접근 권한을 부여합니다.

손 제스처 기반 제어: 로그인 후 활성화되는 손 추적 기능을 통해 마우스 제어, 화면 캡처, 광학 문자 인식(OCR) 등 다양한 PC 작업을 수행합니다.

통합 GUI: 사용자가 시스템의 상태를 쉽게 파악하고 제어할 수 있는 그래픽 사용자 인터페이스(GUI)를 제공합니다.

1.2. 개발 환경
하드웨어: 라즈베리파이 5, 라즈베리파이 카메라 모듈 3

운영체제: Raspberry Pi OS (Debian 기반)

주요 프로그래밍 언어: Python 3

핵심 라이브러리:

AI/컴퓨터 비전: OpenCV, MediaPipe, TFLite Runtime, TensorFlow

GUI 및 입력 제어: Tkinter, Pynput, PyAutoGUI

카메라 제어: Picamera2

2. 시스템 아키텍처 및 구조 (System Architecture)
본 시스템은 여러 독립적인 모듈이 유기적으로 결합된 다중 스레드(Multi-threaded) 애플리케이션으로 설계되었습니다. 이를 통해 무거운 AI 연산과 GUI 업데이트가 서로를 방해하지 않고 부드럽게 작동하도록 했습니다.

2.1. 주요 클래스(모듈) 설명
IntegratedGUI (메인 컨트롤 타워):

Tkinter를 기반으로 한 메인 GUI 창을 생성하고 관리합니다.

전체 시스템의 상태(로그인 전, 로그인 후, 캡처 모드 등)를 총괄합니다.

백그라운드에서 실행되는 영상 처리 스레드를 생성하고 관리합니다.

Queue를 통해 백그라운드 스레드와 안전하게 통신하며, OCR 결과 창 표시와 같은 GUI 관련 작업을 처리합니다.

SharedCamera (통합 카메라 관리자):

단 하나의 Picamera2 객체를 생성하여 얼굴 인식과 손 추적 모듈이 공유하도록 합니다.

별도의 스레드에서 지속적으로 카메라 프레임을 캡처하여 최신 영상을 제공함으로써, 각 모듈이 필요할 때마다 지연 없이 영상에 접근할 수 있습니다.

FaceRecognitionManager (얼굴 인식 관리자):

다중 각도 인식: 한 사람에 대해 정면, 좌측, 우측 등 여러 각도의 얼굴 특징(임베딩)을 데이터베이스(*.pkl)에 저장하여 인식 정확도를 극대화했습니다.

3초 로그인: 등록된 사용자가 3초 동안 꾸준히 인식되어야만 로그인이 성공하도록 하여, 스쳐 지나가는 얼굴에 대한 오인식을 방지합니다.

모델: Lightweight-Face-Detection (탐지) 및 MobileFaceNet (임베딩 추출) TFLite 모델을 사용합니다.

HandTrackingManager (손 추적 및 제스처 관리자):

경로 문제 해결: os.chdir 사용 시 발생했던 라이브러리 충돌 문제를, 모든 모델의 '절대 경로'를 지정하여 로드하는 방식으로 근본적으로 해결했습니다.

지연 로딩(Lazy Loading): 로그인 직후에는 가벼운 손 추적 모델만 먼저 로딩하고, 사용자가 '캡처 모드'에 진입했을 때 비로소 무거운 OCR 모델들을 로딩하여 초기 반응 속도를 높이고 모델 간 충돌을 방지했습니다.

모델: MediaPipe Hands (손 랜드마크 추적), 맞춤형 KeyPointClassifier (제스처 분류), EAST (문자 위치 탐지), CRNN 기반 recognizer_model (문자 인식)을 사용합니다.

3. 핵심 기능 및 구현 (Core Features & Implementation)
3.1. 화면 캡처 및 OCR 워크플로우
사용자 요청 흐름에 맞춰, "손가락으로 화면 캡처 후 해당 이미지 OCR 실행" 기능을 구현했습니다.

모드 진입: Open (손바닥) 제스처로 'Screen Capture & OCR' 모드에 진입합니다.

안정적인 캡처 UI: 이 모드에서는 실시간 영상 대신, 화면 전체를 캡처한 스크린샷이 OpenCV 창에 표시됩니다. 사용자는 이 정지된 화면 위에서 마우스(또는 손가락 제어)로 원하는 영역을 드래그합니다. 드래그하는 동안 실시간으로 빨간색 사각형이 그려져 선택 영역을 명확하게 보여줍니다.

캡처 및 OCR 실행: 마우스 버튼에서 손을 떼면 영역이 확정되고, 키보드 c 키를 누르면 pyautogui가 해당 영역을 캡처하여 이미지 파일로 저장합니다. 저장 직후, 해당 이미지 파일을 입력으로 OCR 기능이 자동으로 실행됩니다.

결과 시각화: OCR이 완료되면, 터미널에 인식된 텍스트가 출력될 뿐만 아니라, 별도의 'OCR Result' 창이 나타나 캡처된 원본 이미지 위에 인식된 글자 영역과 내용이 시각적으로 표시됩니다.

3.2. 문제 해결 과정 (Troubleshooting)
본 프로젝트 개발 과정에서 여러 기술적 난관에 부딪혔으며, 이를 해결한 경험은 다음과 같습니다.

AttributeError (함수 누락): 복잡한 클래스들을 통합하는 과정에서 process_frame과 같은 핵심 함수들이 누락되는 실수가 반복되었습니다. 이는 전체 코드를 한번에 수정하기보다, 기능별로 독립된 테스트 코드를 먼저 성공시킨 후 병합하는 점진적인 개발 방식의 중요성을 깨닫게 했습니다.

라이브러리 충돌: tensorflow와 tflite-runtime을 동시에 사용하거나, os.chdir로 실행 경로를 변경했을 때 MediaPipe가 오작동하는 등 보이지 않는 충돌이 가장 큰 문제였습니다. 이는 **'지연 로딩'**과 **'절대 경로 사용'**이라는 두 가지 원칙을 적용하여 해결했습니다. 필요한 시점에 필요한 모델만 로드하고, 모든 파일 경로는 프로그램의 실행 위치에 상관없이 항상 동일하도록 하여 안정성을 확보했습니다.

캡처 UI 오류 ('회색 화면' 및 '박스 안보임'): Tkinter의 Toplevel을 이용한 투명 오버레이 방식이 라즈베리파이 데스크톱 환경과 호환성 문제를 일으켰습니다. 이 문제는 접근 방식을 완전히 바꿔, OpenCV 창 자체에 스크린샷을 띄우고 cv2.setMouseCallback으로 마우스 이벤트를 직접 제어하는 가장 확실한 방법으로 최종 해결했습니다.

4. 결론 및 향후 과제 (Conclusion)
본 프로젝트를 통해, 라즈베리파이 5라는 소형 컴퓨터에서도 다중 AI 모델과 GUI, 실시간 영상 처리를 통합한 복잡한 애플리케이션을 성공적으로 구축할 수 있음을 확인했습니다. 특히 수많은 버그를 해결하는 과정에서 안정적인 소프트웨어 아키텍처(모듈화, 지연 로딩, 절대 경로)의 중요성을 체감할 수 있었습니다.

향후 개선 과제

기능 확장: 캡처된 텍스트를 클립보드에 복사하거나, 특정 제스처(예: 'OK' 사인)에 새로운 매크로 기능을 할당하는 등 사용자 편의 기능을 추가할 수 있습니다.

성능 최적화: 현재 CPU 기반으로 동작하는 AI 모델들을 라즈베리파이 5의 GPU 가속을 활용하도록 최적화하여 전반적인 반응 속도를 향상시킬 수 있습니다.

사용자 경험(UX) 개선: Tkinter 기반의 시작 창 외에, 모든 시각적 피드백을 OpenCV 창 안에서 통일성 있게 제공하여 더 세련된 사용자 경험을 제공할 수 있습니다.
